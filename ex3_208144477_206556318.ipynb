{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tqUVApFaVvwT","executionInfo":{"status":"ok","timestamp":1687588296481,"user_tz":-180,"elapsed":54521,"user":{"displayName":"snir koska","userId":"04720042086781923593"}},"outputId":"4bca87b3-c91a-403e-f14d-d5a658edde7c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Deep Learning/ex3__208144477_206556318'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eb9MsEorVu45","executionInfo":{"status":"ok","timestamp":1687588448986,"user_tz":-180,"elapsed":282,"user":{"displayName":"snir koska","userId":"04720042086781923593"}},"outputId":"0e31ca7a-0fb1-4a31-8cf9-583d4c9e659b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Deep Learning/ex3__208144477_206556318\n"]}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"QMxyZ0U2OTnZ"},"source":["# Libraries & packages"]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as func\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from sklearn import svm\n","import sklearn.metrics as metrics\n","from prettytable import PrettyTable\n","import pandas as pd\n","import seaborn as sns\n","import warnings\n","\n","\n","my_device = torch.device('cpu')\n","# checking availability of GPU:\n","gpu_available = torch.cuda.is_available()\n","if gpu_available: my_device = torch.device('cuda')\n","print('CUDA is available. Training on GPU' if gpu_available else 'CUDA is unavailable. Training on CPU')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NrkL0f90cn4R","executionInfo":{"status":"ok","timestamp":1687588463103,"user_tz":-180,"elapsed":6497,"user":{"displayName":"snir koska","userId":"04720042086781923593"}},"outputId":"6ff444be-1ba3-499e-fa8b-f204270ea0bf"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA is available. Training on GPU\n"]}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"MSwg9HJWOTnd"},"source":["#Setting default parameter values"]},{"cell_type":"code","source":["#define params:\n","N_EPOCHS = 30\n","in_size = 784 # 28 X 28\n","shape_latent = 10\n","shape_hidden = 256\n","lr = 1e-3\n","random.seed(10)\n","#define criterion:\n","criterion = func.binary_cross_entropy\n","# svm model:\n","svm_model = svm.LinearSVC(random_state=0, tol=1e-5)\n","#mode:\n","mode = 'train'\n","# mode = 'load'"],"metadata":{"id":"81-tCcT4c4pm","executionInfo":{"status":"ok","timestamp":1687588471010,"user_tz":-180,"elapsed":280,"user":{"displayName":"snir koska","userId":"04720042086781923593"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"1ZSSi9IXOTne"},"source":["#Database creation:"]},{"cell_type":"code","source":["def data_init(n_labels, dataset_name):\n","\n","  batch_size = int(n_labels/10)\n","  #download data\n","  if dataset_name =='FashionMNIST':\n","    train_dataset = datasets.FashionMNIST('./data',train=True,download=True,transform=transforms.ToTensor())\n","    test_dataset = datasets.FashionMNIST('./data',train=False,download=True,transform=transforms.ToTensor())\n","  else: #dataset_name =='MNIST':\n","    train_dataset = datasets.MNIST('./data',train=True,download=True,transform=transforms.ToTensor())\n","    test_dataset = datasets.MNIST('./data',train=False,download=True,transform=transforms.ToTensor())\n","  #preparing the data\n","  train_dataset_labeled, not_labeled = torch.utils.data.random_split(train_dataset, [n_labels, len(train_dataset)-n_labels])\n","  train_iter = DataLoader(train_dataset_labeled, batch_size=batch_size, shuffle=True) #reshuffle training data every epoch- reduce model overfitting\n","  test_iter = DataLoader(test_dataset, batch_size=batch_size)\n","\n","  return train_dataset_labeled, train_iter, test_dataset, test_iter\n"],"metadata":{"id":"A88rbJT5eDDx","executionInfo":{"status":"ok","timestamp":1687588472910,"user_tz":-180,"elapsed":279,"user":{"displayName":"snir koska","userId":"04720042086781923593"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"yOzgfUkNOTne"},"source":["#Model: encoder"]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","\n","    def __init__(self, hidden_size, latent_size):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels = 16, kernel_size=3)\n","        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3)\n","        self.linear1 = nn.Linear(32*24*24, 512)\n","        self.linear2 = nn.Linear(512, hidden_size)\n","        self.mean = nn.Linear(hidden_size, latent_size)\n","        self.var = nn.Linear(hidden_size, latent_size)\n","\n","    def forward(self, x):\n","        # x.shape = [batch_size, in_size]\n","        x = func.relu(self.conv1(x))\n","        x = func.relu(self.conv2(x))\n","        bs, ch, w,h = x.shape # bs for batch size\n","        x = x.view(bs,ch*w*h)\n","        # fully connected:\n","        x = func.relu(self.linear1(x))\n","        hidden_layer = func.relu(self.linear2(x))  # hidden_layer.shape = [batch_size, hidden_size]\n","        # latent parameters:\n","        z_mean = self.mean(hidden_layer) # z_mean.shape = [batch_size, latent_size]\n","        z_var_log = self.var(hidden_layer) # z_var_log.shape = [batch_size, latent_size]\n","\n","        return z_mean, z_var_log"],"metadata":{"id":"d6uRy59gea0e","executionInfo":{"status":"ok","timestamp":1687588475531,"user_tz":-180,"elapsed":273,"user":{"displayName":"snir koska","userId":"04720042086781923593"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"7qIkWOvlOTng"},"source":["#Model: decoder"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"1GfYBiJlOTng","executionInfo":{"status":"ok","timestamp":1687588477830,"user_tz":-180,"elapsed":302,"user":{"displayName":"snir koska","userId":"04720042086781923593"}}},"outputs":[],"source":["class Decoder(nn.Module):\n","\n","    def __init__(self, latent_size, hidden_size):\n","        super().__init__()\n","        self.linear = nn.Linear(latent_size, hidden_size)\n","        self.linear2 = nn.Linear(hidden_size, 512)\n","        self.linear3 = nn.Linear(512, 18432)\n","        self.deconv1 = nn.ConvTranspose2d(32, 16, kernel_size = 3)\n","        self.deconv2 = nn.ConvTranspose2d(16, 1, kernel_size = 3)\n","\n","    def forward(self, x):  # x.shape = [batch_size, shape_latent]\n","        x = func.relu(self.linear(x))\n","        x = func.relu(self.linear2(x))\n","        x = func.relu(self.linear3(x))\n","        x = x.view(-1, 32, 24,24)\n","        x = func.relu(self.deconv1(x))\n","        output = torch.sigmoid(self.deconv2(x)) # output.shape = [batch_size, output_dim]\n","\n","        return output"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"caklkiNNOTnh"},"source":["#Model: VAE"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"-20F3ocHOTnh","executionInfo":{"status":"ok","timestamp":1687588479803,"user_tz":-180,"elapsed":4,"user":{"displayName":"snir koska","userId":"04720042086781923593"}}},"outputs":[],"source":["class VAE(nn.Module):\n","\n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, x):\n","        # encode x\n","        # latent params:\n","        z_mean, z_var_log = self.encoder(x)\n","        # sampling from standard normal dist:\n","        std = torch.exp(z_var_log/2)\n","        normal_sample = torch.randn_like(std)\n","        x_sample = normal_sample.mul(std).add_(z_mean) # ~N(z_mean, std)\n","\n","        # decode\n","        output = self.decoder(x_sample)\n","        return output, z_mean, z_var_log"]},{"cell_type":"code","source":["#weights initialization\n","#applies Xavier uniform initialization to the weights of linear layers in a neural network\n","def init_xavier(m):\n","    if type(m) == nn.Linear:\n","        nn.init.xavier_uniform_(m.weight)"],"metadata":{"id":"x6iJwXyc9L5E","executionInfo":{"status":"ok","timestamp":1687588482916,"user_tz":-180,"elapsed":276,"user":{"displayName":"snir koska","userId":"04720042086781923593"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"n0pK_BgYOTni"},"source":["# Train & test proccesses:"]},{"cell_type":"code","source":["def train_model(model, train_iterator):\n","\n","    model.train()\n","    train_loss = 0\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    for data, target in train_iterator:\n","        data = data.to(my_device)\n","        optimizer.zero_grad()\n","        # forward pass\n","        x_sample, z_mean, z_var_log = model(data)\n","        # reconstruction loss\n","        BCE_loss = criterion(x_sample, data, size_average=False)\n","        # kl divergence loss-\n","        # wrt kl div of gaussian dists: 0.5*log(var2/var1) + 0.5*(var1+(miu1-miu2)^2)/var2 - 0.5\n","        kl_loss = 0.5 * torch.sum(-z_var_log + torch.exp(z_var_log) + z_mean**2 - 1.0)\n","        loss = kl_loss + BCE_loss\n","        # backward pass\n","        loss.backward()\n","        optimizer.step() # update weights\n","        train_loss += loss.item()\n","\n","    return train_loss"],"metadata":{"id":"5e0qTIzycBDp","executionInfo":{"status":"ok","timestamp":1687588486667,"user_tz":-180,"elapsed":297,"user":{"displayName":"snir koska","userId":"04720042086781923593"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def test_model(model, test_iterator):\n","\n","    model.eval()\n","    test_loss = 0\n","    for data, target in test_iterator:\n","        data = data.to(my_device)\n","        # forward\n","        with torch.no_grad(): # turn off grad computation during evaluation\n","          x_sample, z_mean, z_var_log = model(data)\n","        # reconstruction loss\n","        BCE_loss = criterion(x_sample, data, size_average=False)\n","        # kl divergence loss\n","        # wrt kl div of gaussian dists: 0.5*log(var2/var1) + 0.5*(var1+(miu1-miu2)^2)/var2 - 0.5\n","        kl_loss = 0.5 * torch.sum(-z_var_log + torch.exp(z_var_log) + z_mean**2 - 1.0)\n","        loss = kl_loss + BCE_loss\n","        test_loss += loss.item()\n","\n","    return test_loss"],"metadata":{"id":"eR8xtMi0azSa","executionInfo":{"status":"ok","timestamp":1687588488601,"user_tz":-180,"elapsed":5,"user":{"displayName":"snir koska","userId":"04720042086781923593"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def fit(model, batch_size, n_labels, train_dataset, train_iterator, test_dataset, test_iterator):\n","\n","    min_test_loss = float('inf')\n","    for epoch in range(N_EPOCHS):\n","        train_loss = train_model(model, train_iterator)\n","        test_loss = test_model(model, test_iterator)\n","        train_loss /= len(train_dataset)\n","        test_loss /= len(test_dataset)\n","        print(f'Epoch: {epoch}, Training Loss: {train_loss:.2f}, Testing Loss: {test_loss:.2f}')\n","        # defining tolerance:\n","        if min_test_loss > test_loss:\n","            min_test_loss = test_loss\n","            patience = 1\n","        else:\n","            patience += 1\n","        if patience > 4:\n","            break\n","    print(\"\\n\")\n","    # save NN weights\n","    torch.save(model.state_dict(), f'model_VAE_{n_labels}_labels.pt')"],"metadata":{"id":"i22uDZo5Zw7N","executionInfo":{"status":"ok","timestamp":1687588490576,"user_tz":-180,"elapsed":284,"user":{"displayName":"snir koska","userId":"04720042086781923593"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"RHWH0sHCOTnk"},"source":["# Generating latent representation:"]},{"cell_type":"code","source":["def latent_data(model, db_iterator):\n","\n","    check_first = 0\n","    db_labels = []\n","\n","    with torch.no_grad():\n","        for batch, labels in db_iterator:\n","            batch = batch.to(my_device)\n","            labels = labels.tolist()\n","            # concatenate all batches labels:\n","            db_labels.extend(labels)\n","            #Pass the batch through encoder model-get latent represent\n","            latent_mean, latent_var = model.encoder(batch)\n","            # '.numpy()' method does not support CUDA tensors\n","            latent_mean = latent_mean.to('cpu').numpy()\n","            latent_var = latent_var.to('cpu').numpy()\n","            if check_first == 0:\n","              db_means = latent_mean\n","              db_variances = latent_var\n","              check_first = 1\n","            else:\n","              # stack all batches (latent representation) params:\n","              db_means = np.vstack((db_means, latent_mean))\n","              db_variances = np.vstack((db_variances, latent_var))\n","\n","\n","    return db_means, db_variances, db_labels"],"metadata":{"id":"8ZADC_FlQYg3","executionInfo":{"status":"ok","timestamp":1687588494425,"user_tz":-180,"elapsed":294,"user":{"displayName":"snir koska","userId":"04720042086781923593"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# Classification model: SVM\n","\n"],"metadata":{"id":"jcYo-2mrBHBr"}},{"cell_type":"code","source":["def SVM_classification(model, n_labels, train_means, train_vars, train_labels, test_iter):\n","\n","    # train the SVM classifier:\n","    #preparing latent database to train SVM classifier:\n","    train_latent_data = np.column_stack((train_means, train_vars))\n","    train_latent_df = pd.DataFrame(train_latent_data)\n","    train_labels = np.array(train_labels)\n","\n","    # train svm classifier:\n","    if mode == 'train':\n","      svm_model.fit(train_latent_df, train_labels)\n","      # save SVM model\n","      torch.save(svm_model, f'model_SVM_{n_labels}_labels.pt')\n","    # loading trained model for prediction:\n","    svm_model_trained = torch.load(f'model_SVM_{n_labels}_labels.pt')\n","\n","    #preparing test latent database:\n","    test_means, test_vars, test_labels = latent_data(model, test_iter)\n","    test_latent_data = np.column_stack((test_means, test_vars))\n","    test_latent_df = pd.DataFrame(test_latent_data)\n","    test_labels = np.array(test_labels)\n","    # prediction:\n","    prediction = svm_model_trained.predict(test_latent_df)\n","\n","    return prediction, test_labels"],"metadata":{"id":"kPAB8AEfgzcA","executionInfo":{"status":"ok","timestamp":1687588497886,"user_tz":-180,"elapsed":287,"user":{"displayName":"snir koska","userId":"04720042086781923593"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["# Main function:"],"metadata":{"id":"o9Uag4AyBcYa"}},{"cell_type":"code","source":["def run_main(n_labels, dataset_name):\n","\n","  batch_size = int(n_labels/10)\n","  train_dataset_labeled, train_iter, test_dataset, test_iter = data_init(n_labels,dataset_name)\n","  # model\n","  encoder = Encoder(shape_hidden, shape_latent)\n","  decoder = Decoder(shape_latent, shape_hidden)\n","  model = VAE(encoder, decoder)\n","  model = model.to(my_device)\n","  model.apply(init_xavier);\n","\n","  print(f'{dataset_name} dataset with {n_labels} labels:\\n')\n","\n","  # train:\n","  fit(model, batch_size, n_labels, train_dataset_labeled, train_iter, test_dataset, test_iter)\n","  # load model weights:\n","  model.load_state_dict(torch.load(f'model_VAE_{n_labels}_labels.pt'))\n","\n","  train_means, train_vars, train_labels = latent_data(model, train_iter)\n","  ypreds, test_labels = SVM_classification(model, n_labels, train_means, train_vars, train_labels, test_iter)\n","\n","  # assessment metrics to analyze the classification algorithm:\n","  # accuracy: total correctly classified example divided by total number of classified examples.\n","  test_accuracy = metrics.accuracy_score(test_labels, ypreds)\n","  f_score = metrics.f1_score(test_labels, ypreds, average='macro') #F1 Score = 2*(Recall * Precision)/(Recall + Precision)\n","  precision = metrics.precision_score(test_labels, ypreds, average='macro')  #precision= TP/(TP+FP)\n","  recall = metrics.recall_score(test_labels, ypreds, average='macro') #recall  = TP/(TP+FN)\n","  print('Test Accuracy : ', test_accuracy)\n","  print('Test Data f-Score : ', f_score)\n","  print('Test Recall : ', recall)\n","  print('Test prec : ', precision)\n","  print(\"*************************************************\\n\\n\")\n","\n","\n","  return test_accuracy*100"],"metadata":{"id":"mQD7K99TiLxN","executionInfo":{"status":"ok","timestamp":1687588526537,"user_tz":-180,"elapsed":301,"user":{"displayName":"snir koska","userId":"04720042086781923593"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["# Results:"],"metadata":{"id":"veQcjgXpDQEC"}},{"cell_type":"code","source":["warnings.filterwarnings('ignore')\n","\n","test_acc100_fashion = run_main(n_labels=100, dataset_name ='FashionMNIST')\n","test_acc600_fashion = run_main(n_labels=600, dataset_name ='FashionMNIST')\n","test_acc1000_fashion = run_main(n_labels=1000, dataset_name ='FashionMNIST')\n","test_acc3000_fashion = run_main(n_labels=3000, dataset_name ='FashionMNIST')\n","\n","test_acc100 = run_main(n_labels=100, dataset_name ='MNIST')\n","test_acc600 = run_main(n_labels=600, dataset_name ='MNIST')\n","test_acc1000 = run_main(n_labels=1000, dataset_name ='MNIST')\n","test_acc3000 = run_main(n_labels=3000, dataset_name ='MNIST')"],"metadata":{"id":"Ax-GmnGxqr7J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687589094416,"user_tz":-180,"elapsed":561448,"user":{"displayName":"snir koska","userId":"04720042086781923593"}},"outputId":"e8b023c0-62f0-4960-8c76-633bdb5b3904"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["FashionMNIST dataset with 100 labels:\n","\n","Epoch: 0, Training Loss: 480.39, Testing Loss: 422.12\n","Epoch: 1, Training Loss: 396.23, Testing Loss: 380.13\n","Epoch: 2, Training Loss: 358.40, Testing Loss: 344.11\n","Epoch: 3, Training Loss: 327.07, Testing Loss: 332.95\n","Epoch: 4, Training Loss: 316.43, Testing Loss: 329.14\n","Epoch: 5, Training Loss: 313.57, Testing Loss: 327.49\n","Epoch: 6, Training Loss: 304.80, Testing Loss: 316.76\n","Epoch: 7, Training Loss: 304.00, Testing Loss: 322.43\n","Epoch: 8, Training Loss: 306.72, Testing Loss: 321.76\n","Epoch: 9, Training Loss: 302.91, Testing Loss: 312.97\n","Epoch: 10, Training Loss: 300.95, Testing Loss: 309.84\n","Epoch: 11, Training Loss: 301.52, Testing Loss: 313.13\n","Epoch: 12, Training Loss: 298.32, Testing Loss: 310.33\n","Epoch: 13, Training Loss: 294.72, Testing Loss: 308.01\n","Epoch: 14, Training Loss: 296.47, Testing Loss: 308.19\n","Epoch: 15, Training Loss: 293.61, Testing Loss: 308.65\n","Epoch: 16, Training Loss: 296.07, Testing Loss: 308.59\n","Epoch: 17, Training Loss: 289.72, Testing Loss: 309.37\n","\n","\n","Test Accuracy :  0.5976\n","Test Data f-Score :  0.5622435522772293\n","Test Recall :  0.5975999999999999\n","Test prec :  0.572350077980876\n","*************************************************\n","\n","\n","FashionMNIST dataset with 600 labels:\n","\n","Epoch: 0, Training Loss: 486.68, Testing Loss: 436.47\n","Epoch: 1, Training Loss: 409.04, Testing Loss: 376.95\n","Epoch: 2, Training Loss: 361.43, Testing Loss: 331.00\n","Epoch: 3, Training Loss: 337.48, Testing Loss: 321.63\n","Epoch: 4, Training Loss: 329.52, Testing Loss: 315.72\n","Epoch: 5, Training Loss: 317.76, Testing Loss: 306.51\n","Epoch: 6, Training Loss: 314.31, Testing Loss: 303.90\n","Epoch: 7, Training Loss: 307.17, Testing Loss: 299.02\n","Epoch: 8, Training Loss: 301.81, Testing Loss: 296.06\n","Epoch: 9, Training Loss: 300.76, Testing Loss: 294.18\n","Epoch: 10, Training Loss: 301.95, Testing Loss: 294.76\n","Epoch: 11, Training Loss: 295.02, Testing Loss: 286.80\n","Epoch: 12, Training Loss: 291.19, Testing Loss: 286.05\n","Epoch: 13, Training Loss: 289.10, Testing Loss: 282.11\n","Epoch: 14, Training Loss: 286.70, Testing Loss: 282.42\n","Epoch: 15, Training Loss: 288.14, Testing Loss: 281.50\n","Epoch: 16, Training Loss: 281.82, Testing Loss: 276.55\n","Epoch: 17, Training Loss: 280.19, Testing Loss: 275.72\n","Epoch: 18, Training Loss: 280.33, Testing Loss: 277.36\n","Epoch: 19, Training Loss: 283.86, Testing Loss: 278.09\n","Epoch: 20, Training Loss: 278.54, Testing Loss: 275.09\n","Epoch: 21, Training Loss: 275.68, Testing Loss: 272.35\n","Epoch: 22, Training Loss: 272.75, Testing Loss: 271.34\n","Epoch: 23, Training Loss: 273.19, Testing Loss: 272.07\n","Epoch: 24, Training Loss: 275.17, Testing Loss: 272.28\n","Epoch: 25, Training Loss: 274.41, Testing Loss: 271.50\n","Epoch: 26, Training Loss: 269.63, Testing Loss: 269.11\n","Epoch: 27, Training Loss: 267.33, Testing Loss: 267.95\n","Epoch: 28, Training Loss: 269.10, Testing Loss: 270.23\n","Epoch: 29, Training Loss: 269.26, Testing Loss: 267.13\n","\n","\n","Test Accuracy :  0.7255\n","Test Data f-Score :  0.7135067389157855\n","Test Recall :  0.7255\n","Test prec :  0.7122558583394483\n","*************************************************\n","\n","\n","FashionMNIST dataset with 1000 labels:\n","\n","Epoch: 0, Training Loss: 462.70, Testing Loss: 405.48\n","Epoch: 1, Training Loss: 386.37, Testing Loss: 356.47\n","Epoch: 2, Training Loss: 340.40, Testing Loss: 319.28\n","Epoch: 3, Training Loss: 326.01, Testing Loss: 313.89\n","Epoch: 4, Training Loss: 316.41, Testing Loss: 306.83\n","Epoch: 5, Training Loss: 315.34, Testing Loss: 307.08\n","Epoch: 6, Training Loss: 311.48, Testing Loss: 304.48\n","Epoch: 7, Training Loss: 302.79, Testing Loss: 296.40\n","Epoch: 8, Training Loss: 297.49, Testing Loss: 290.96\n","Epoch: 9, Training Loss: 294.25, Testing Loss: 288.32\n","Epoch: 10, Training Loss: 291.68, Testing Loss: 287.74\n","Epoch: 11, Training Loss: 287.29, Testing Loss: 282.24\n","Epoch: 12, Training Loss: 284.81, Testing Loss: 279.62\n","Epoch: 13, Training Loss: 282.26, Testing Loss: 277.61\n","Epoch: 14, Training Loss: 279.03, Testing Loss: 277.79\n","Epoch: 15, Training Loss: 277.74, Testing Loss: 275.07\n","Epoch: 16, Training Loss: 284.97, Testing Loss: 280.64\n","Epoch: 17, Training Loss: 277.27, Testing Loss: 274.65\n","Epoch: 18, Training Loss: 275.74, Testing Loss: 273.63\n","Epoch: 19, Training Loss: 275.94, Testing Loss: 274.31\n","Epoch: 20, Training Loss: 272.51, Testing Loss: 271.46\n","Epoch: 21, Training Loss: 270.87, Testing Loss: 268.93\n","Epoch: 22, Training Loss: 270.63, Testing Loss: 269.24\n","Epoch: 23, Training Loss: 269.69, Testing Loss: 269.87\n","Epoch: 24, Training Loss: 267.80, Testing Loss: 267.83\n","Epoch: 25, Training Loss: 269.09, Testing Loss: 268.00\n","Epoch: 26, Training Loss: 269.36, Testing Loss: 266.46\n","Epoch: 27, Training Loss: 265.23, Testing Loss: 266.06\n","Epoch: 28, Training Loss: 265.13, Testing Loss: 265.73\n","Epoch: 29, Training Loss: 264.28, Testing Loss: 264.69\n","\n","\n","Test Accuracy :  0.7236\n","Test Data f-Score :  0.7199652680036394\n","Test Recall :  0.7236\n","Test prec :  0.7224905788511307\n","*************************************************\n","\n","\n","FashionMNIST dataset with 3000 labels:\n","\n","Epoch: 0, Training Loss: 516.50, Testing Loss: 430.07\n","Epoch: 1, Training Loss: 427.81, Testing Loss: 402.75\n","Epoch: 2, Training Loss: 389.30, Testing Loss: 371.98\n","Epoch: 3, Training Loss: 381.33, Testing Loss: 369.44\n","Epoch: 4, Training Loss: 356.71, Testing Loss: 333.68\n","Epoch: 5, Training Loss: 324.80, Testing Loss: 312.96\n","Epoch: 6, Training Loss: 315.51, Testing Loss: 307.43\n","Epoch: 7, Training Loss: 312.18, Testing Loss: 303.98\n","Epoch: 8, Training Loss: 309.39, Testing Loss: 298.93\n","Epoch: 9, Training Loss: 301.30, Testing Loss: 294.25\n","Epoch: 10, Training Loss: 298.57, Testing Loss: 292.14\n","Epoch: 11, Training Loss: 297.59, Testing Loss: 287.49\n","Epoch: 12, Training Loss: 290.00, Testing Loss: 283.50\n","Epoch: 13, Training Loss: 294.27, Testing Loss: 282.87\n","Epoch: 14, Training Loss: 283.45, Testing Loss: 278.62\n","Epoch: 15, Training Loss: 283.82, Testing Loss: 276.69\n","Epoch: 16, Training Loss: 281.85, Testing Loss: 277.97\n","Epoch: 17, Training Loss: 279.34, Testing Loss: 274.11\n","Epoch: 18, Training Loss: 281.36, Testing Loss: 274.53\n","Epoch: 19, Training Loss: 279.83, Testing Loss: 274.24\n","Epoch: 20, Training Loss: 276.66, Testing Loss: 272.11\n","Epoch: 21, Training Loss: 273.17, Testing Loss: 269.71\n","Epoch: 22, Training Loss: 271.46, Testing Loss: 268.44\n","Epoch: 23, Training Loss: 275.27, Testing Loss: 267.54\n","Epoch: 24, Training Loss: 273.36, Testing Loss: 268.52\n","Epoch: 25, Training Loss: 268.16, Testing Loss: 265.71\n","Epoch: 26, Training Loss: 273.82, Testing Loss: 267.06\n","Epoch: 27, Training Loss: 266.69, Testing Loss: 264.24\n","Epoch: 28, Training Loss: 270.05, Testing Loss: 264.89\n","Epoch: 29, Training Loss: 267.39, Testing Loss: 265.44\n","\n","\n","Test Accuracy :  0.7431\n","Test Data f-Score :  0.7344296411313298\n","Test Recall :  0.7430999999999999\n","Test prec :  0.7318470446909717\n","*************************************************\n","\n","\n","MNIST dataset with 100 labels:\n","\n","Epoch: 0, Training Loss: 363.77, Testing Loss: 258.21\n","Epoch: 1, Training Loss: 228.88, Testing Loss: 216.92\n","Epoch: 2, Training Loss: 211.48, Testing Loss: 212.15\n","Epoch: 3, Training Loss: 206.69, Testing Loss: 205.36\n","Epoch: 4, Training Loss: 201.93, Testing Loss: 204.02\n","Epoch: 5, Training Loss: 198.11, Testing Loss: 202.15\n","Epoch: 6, Training Loss: 195.22, Testing Loss: 198.79\n","Epoch: 7, Training Loss: 193.04, Testing Loss: 195.62\n","Epoch: 8, Training Loss: 187.95, Testing Loss: 194.87\n","Epoch: 9, Training Loss: 187.52, Testing Loss: 194.02\n","Epoch: 10, Training Loss: 186.07, Testing Loss: 193.60\n","Epoch: 11, Training Loss: 183.96, Testing Loss: 191.59\n","Epoch: 12, Training Loss: 179.69, Testing Loss: 189.62\n","Epoch: 13, Training Loss: 176.99, Testing Loss: 186.97\n","Epoch: 14, Training Loss: 175.17, Testing Loss: 187.02\n","Epoch: 15, Training Loss: 170.99, Testing Loss: 183.01\n","Epoch: 16, Training Loss: 170.57, Testing Loss: 182.59\n","Epoch: 17, Training Loss: 168.15, Testing Loss: 183.03\n","Epoch: 18, Training Loss: 166.42, Testing Loss: 181.83\n","Epoch: 19, Training Loss: 168.42, Testing Loss: 182.89\n","Epoch: 20, Training Loss: 164.32, Testing Loss: 180.48\n","Epoch: 21, Training Loss: 164.93, Testing Loss: 180.84\n","Epoch: 22, Training Loss: 159.58, Testing Loss: 182.52\n","Epoch: 23, Training Loss: 160.64, Testing Loss: 179.34\n","Epoch: 24, Training Loss: 159.58, Testing Loss: 177.83\n","Epoch: 25, Training Loss: 156.25, Testing Loss: 176.91\n","Epoch: 26, Training Loss: 160.57, Testing Loss: 176.36\n","Epoch: 27, Training Loss: 154.36, Testing Loss: 177.62\n","Epoch: 28, Training Loss: 154.32, Testing Loss: 176.01\n","Epoch: 29, Training Loss: 153.79, Testing Loss: 174.41\n","\n","\n","Test Accuracy :  0.6429\n","Test Data f-Score :  0.631033570305797\n","Test Recall :  0.6386383242582897\n","Test prec :  0.6882616278082642\n","*************************************************\n","\n","\n","MNIST dataset with 600 labels:\n","\n","Epoch: 0, Training Loss: 338.47, Testing Loss: 241.14\n","Epoch: 1, Training Loss: 221.89, Testing Loss: 210.47\n","Epoch: 2, Training Loss: 207.78, Testing Loss: 203.34\n","Epoch: 3, Training Loss: 203.68, Testing Loss: 199.61\n","Epoch: 4, Training Loss: 200.22, Testing Loss: 196.01\n","Epoch: 5, Training Loss: 196.95, Testing Loss: 194.91\n","Epoch: 6, Training Loss: 195.68, Testing Loss: 193.97\n","Epoch: 7, Training Loss: 192.92, Testing Loss: 190.12\n","Epoch: 8, Training Loss: 188.53, Testing Loss: 183.39\n","Epoch: 9, Training Loss: 181.50, Testing Loss: 177.00\n","Epoch: 10, Training Loss: 178.33, Testing Loss: 174.21\n","Epoch: 11, Training Loss: 175.31, Testing Loss: 172.02\n","Epoch: 12, Training Loss: 173.67, Testing Loss: 170.82\n","Epoch: 13, Training Loss: 172.62, Testing Loss: 170.04\n","Epoch: 14, Training Loss: 170.20, Testing Loss: 167.47\n","Epoch: 15, Training Loss: 166.47, Testing Loss: 165.60\n","Epoch: 16, Training Loss: 164.31, Testing Loss: 163.83\n","Epoch: 17, Training Loss: 163.66, Testing Loss: 163.36\n","Epoch: 18, Training Loss: 161.67, Testing Loss: 161.82\n","Epoch: 19, Training Loss: 160.41, Testing Loss: 160.01\n","Epoch: 20, Training Loss: 157.21, Testing Loss: 156.57\n","Epoch: 21, Training Loss: 155.04, Testing Loss: 155.09\n","Epoch: 22, Training Loss: 154.39, Testing Loss: 153.19\n","Epoch: 23, Training Loss: 151.10, Testing Loss: 151.51\n","Epoch: 24, Training Loss: 150.29, Testing Loss: 150.80\n","Epoch: 25, Training Loss: 150.07, Testing Loss: 151.18\n","Epoch: 26, Training Loss: 147.98, Testing Loss: 149.48\n","Epoch: 27, Training Loss: 146.42, Testing Loss: 148.24\n","Epoch: 28, Training Loss: 144.88, Testing Loss: 147.23\n","Epoch: 29, Training Loss: 144.37, Testing Loss: 146.90\n","\n","\n","Test Accuracy :  0.8237\n","Test Data f-Score :  0.8185870636818461\n","Test Recall :  0.8191283346572484\n","Test prec :  0.8221693434138622\n","*************************************************\n","\n","\n","MNIST dataset with 1000 labels:\n","\n","Epoch: 0, Training Loss: 384.66, Testing Loss: 271.02\n","Epoch: 1, Training Loss: 242.76, Testing Loss: 216.68\n","Epoch: 2, Training Loss: 215.22, Testing Loss: 205.23\n","Epoch: 3, Training Loss: 205.02, Testing Loss: 198.23\n","Epoch: 4, Training Loss: 205.50, Testing Loss: 199.10\n","Epoch: 5, Training Loss: 199.84, Testing Loss: 194.71\n","Epoch: 6, Training Loss: 195.87, Testing Loss: 189.64\n","Epoch: 7, Training Loss: 194.46, Testing Loss: 187.29\n","Epoch: 8, Training Loss: 188.33, Testing Loss: 183.43\n","Epoch: 9, Training Loss: 185.91, Testing Loss: 182.27\n","Epoch: 10, Training Loss: 184.25, Testing Loss: 181.17\n","Epoch: 11, Training Loss: 182.37, Testing Loss: 178.01\n","Epoch: 12, Training Loss: 176.48, Testing Loss: 170.60\n","Epoch: 13, Training Loss: 173.45, Testing Loss: 169.05\n","Epoch: 14, Training Loss: 169.26, Testing Loss: 165.30\n","Epoch: 15, Training Loss: 166.21, Testing Loss: 161.23\n","Epoch: 16, Training Loss: 162.76, Testing Loss: 157.94\n","Epoch: 17, Training Loss: 159.72, Testing Loss: 156.45\n","Epoch: 18, Training Loss: 156.80, Testing Loss: 152.81\n","Epoch: 19, Training Loss: 155.98, Testing Loss: 153.36\n","Epoch: 20, Training Loss: 153.07, Testing Loss: 149.67\n","Epoch: 21, Training Loss: 151.75, Testing Loss: 149.59\n","Epoch: 22, Training Loss: 149.69, Testing Loss: 148.01\n","Epoch: 23, Training Loss: 149.98, Testing Loss: 147.52\n","Epoch: 24, Training Loss: 148.84, Testing Loss: 147.50\n","Epoch: 25, Training Loss: 148.68, Testing Loss: 147.51\n","Epoch: 26, Training Loss: 145.00, Testing Loss: 144.49\n","Epoch: 27, Training Loss: 144.25, Testing Loss: 144.36\n","Epoch: 28, Training Loss: 143.21, Testing Loss: 143.24\n","Epoch: 29, Training Loss: 142.53, Testing Loss: 142.37\n","\n","\n","Test Accuracy :  0.8389\n","Test Data f-Score :  0.8354610642026044\n","Test Recall :  0.835806829687838\n","Test prec :  0.8373239197587579\n","*************************************************\n","\n","\n","MNIST dataset with 3000 labels:\n","\n","Epoch: 0, Training Loss: 469.10, Testing Loss: 299.05\n","Epoch: 1, Training Loss: 254.37, Testing Loss: 219.82\n","Epoch: 2, Training Loss: 223.54, Testing Loss: 212.28\n","Epoch: 3, Training Loss: 211.02, Testing Loss: 203.61\n","Epoch: 4, Training Loss: 211.80, Testing Loss: 205.49\n","Epoch: 5, Training Loss: 203.11, Testing Loss: 196.57\n","Epoch: 6, Training Loss: 204.01, Testing Loss: 198.21\n","Epoch: 7, Training Loss: 199.35, Testing Loss: 193.88\n","Epoch: 8, Training Loss: 198.89, Testing Loss: 193.95\n","Epoch: 9, Training Loss: 196.07, Testing Loss: 191.91\n","Epoch: 10, Training Loss: 196.46, Testing Loss: 191.73\n","Epoch: 11, Training Loss: 195.88, Testing Loss: 190.62\n","Epoch: 12, Training Loss: 193.95, Testing Loss: 188.70\n","Epoch: 13, Training Loss: 191.53, Testing Loss: 186.22\n","Epoch: 14, Training Loss: 188.44, Testing Loss: 181.92\n","Epoch: 15, Training Loss: 184.70, Testing Loss: 179.03\n","Epoch: 16, Training Loss: 181.06, Testing Loss: 176.22\n","Epoch: 17, Training Loss: 177.15, Testing Loss: 170.75\n","Epoch: 18, Training Loss: 172.84, Testing Loss: 166.86\n","Epoch: 19, Training Loss: 168.77, Testing Loss: 163.67\n","Epoch: 20, Training Loss: 166.26, Testing Loss: 161.79\n","Epoch: 21, Training Loss: 163.81, Testing Loss: 159.16\n","Epoch: 22, Training Loss: 161.52, Testing Loss: 157.19\n","Epoch: 23, Training Loss: 161.27, Testing Loss: 156.64\n","Epoch: 24, Training Loss: 159.20, Testing Loss: 155.51\n","Epoch: 25, Training Loss: 159.21, Testing Loss: 153.92\n","Epoch: 26, Training Loss: 158.33, Testing Loss: 153.64\n","Epoch: 27, Training Loss: 155.20, Testing Loss: 151.96\n","Epoch: 28, Training Loss: 154.45, Testing Loss: 151.36\n","Epoch: 29, Training Loss: 153.15, Testing Loss: 149.61\n","\n","\n","Test Accuracy :  0.8243\n","Test Data f-Score :  0.817360322764481\n","Test Recall :  0.8191191187775166\n","Test prec :  0.8208617692430605\n","*************************************************\n","\n","\n"]}]},{"cell_type":"markdown","source":["# Results table:\n"],"metadata":{"id":"uWTQguLTCmLZ"}},{"cell_type":"code","source":["myTable = PrettyTable([\"Dataset\", \"# of labels in train set\", \"Test Accuracy\"])\n","\n","myTable.add_row([\"FashionMNIST\", \"100\", f\"{round(test_acc100_fashion,2)}\"])\n","myTable.add_row([\"FashionMNIST\", \"600\", f\"{round(test_acc600_fashion,2)}\"])\n","myTable.add_row([\"FashionMNIST\", \"1000\", f\"{round(test_acc1000_fashion,2)}\"])\n","myTable.add_row([\"FashionMNIST\", \"3000\", f\"{round(test_acc3000_fashion,2)}\"])\n","myTable.add_row([\"MNIST\", \"100\", f\"{round(test_acc100,2)}\"])\n","myTable.add_row([\"MNIST\", \"600\", f\"{round(test_acc600,2)}\"])\n","myTable.add_row([\"MNIST\", \"1000\", f\"{round(test_acc1000,2)}\"])\n","myTable.add_row([\"MNIST\", \"3000\", f\"{round(test_acc3000,2)}\"])\n","\n","print(\"Results Summary - different datasets and number of labels in training\")\n","print(myTable)"],"metadata":{"id":"dYwuwmfWrLLi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687589254214,"user_tz":-180,"elapsed":293,"user":{"displayName":"snir koska","userId":"04720042086781923593"}},"outputId":"9bcbadf4-dae6-4e08-e74a-42f90d536fc7"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Results Summary - different datasets and number of labels in training\n","+--------------+--------------------------+---------------+\n","|   Dataset    | # of labels in train set | Test Accuracy |\n","+--------------+--------------------------+---------------+\n","| FashionMNIST |           100            |     59.76     |\n","| FashionMNIST |           600            |     72.55     |\n","| FashionMNIST |           1000           |     72.36     |\n","| FashionMNIST |           3000           |     74.31     |\n","|    MNIST     |           100            |     64.29     |\n","|    MNIST     |           600            |     82.37     |\n","|    MNIST     |           1000           |     83.89     |\n","|    MNIST     |           3000           |     82.43     |\n","+--------------+--------------------------+---------------+\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1BlosnnSFk62FSlT1xD9Ur3g9umxLJZDV","timestamp":1639251300871}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"}},"nbformat":4,"nbformat_minor":0}